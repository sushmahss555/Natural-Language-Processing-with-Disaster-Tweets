# ğŸŒ€ Natural Language Processing with Disaster Tweets (RNN Model)

This project is part of the weekly mini-project assignment.  
We participate in the [Kaggle competition: Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started), where the goal is to classify tweets as either disaster-related (`1`) or not disaster-related (`0`).

---

## ğŸ“Œ Project Overview
- **Problem:** Given short text tweets, predict whether the text is about a real disaster.  
- **Approach:** Preprocess text, convert tweets into sequences using tokenization, and train a **Recurrent Neural Network (RNN)** for binary classification.  
- **Deliverables:**
  1. Jupyter Notebook with code, analysis, and discussion.  
  2. GitHub repository (this repo).  
  3. Kaggle leaderboard screenshot with submission score.  

---

## ğŸ“Š Dataset
- **train.csv** â€“ contains tweets and labels (`target` = 0 or 1).  
- **test.csv** â€“ contains tweets (labels not provided).  
- **sample_submission.csv** â€“ example format for Kaggle submission.  

**Dataset Structure (train.csv):**
- `id` â€“ unique identifier for each tweet  
- `text` â€“ the actual tweet content  
- `target` â€“ 1 = disaster, 0 = not disaster  

---

## ğŸ” Exploratory Data Analysis (EDA)
- Checked dataset shape, missing values, and class balance.  
- Visualized label distribution (`target=0` vs `target=1`).  
- Token length analysis (average ~15 words per tweet).  
- Basic text cleaning (lowercasing, tokenization, padding).  

---

## ğŸ—ï¸ Model Architecture
A **vanilla Recurrent Neural Network (RNN)** is used:

1. **Embedding Layer** â€“ maps words to dense vectors.  
2. **Simple RNN Layer** â€“ processes sequences step-by-step.  
3. **Dropout** â€“ prevents overfitting.  
4. **Dense Layers** â€“ classification layers.  
5. **Output Layer** â€“ sigmoid activation for binary classification.  

```python
model = Sequential([
    Embedding(input_dim=max_words, output_dim=100, input_length=max_len),
    SimpleRNN(64, activation='tanh'),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])
